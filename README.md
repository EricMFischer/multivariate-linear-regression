## Synopsis
In this exercise, I implemented multivariate linear regression with the gradient descent algorithm and the normal equation as two ways to minimize the cost function J(theta). Also included here is an exercise of preprocessing feature normalization. 

Files included in this exercise:
ex1.m - Octave/MATLAB script that steps through the exercise
ex1 multi.m - Octave/MATLAB script for the later parts of the exercise
ex1data1.txt - Dataset for linear regression with one variable
ex1data2.txt - Dataset for linear regression with multiple variables
submit.m - Submission script that sends your solutions to our servers
[?] warmUpExercise.m - Simple example function in Octave/MATLAB
[?] plotData.m - Function to display the dataset
[?] computeCost.m - Function to compute the cost of linear regression
[?] gradientDescent.m - Function to run gradient descent
[y] computeCostMulti.m - Cost function for multiple variables
[y] gradientDescentMulti.m - Gradient descent for multiple variables
[y] featureNormalize.m - Function to normalize features
[y] normalEqn.m - Function to compute the normal equations
? indicates necessary exercises
y indicates optional exercises

Throughout the exercise, the scripts ex1.m and ex1 multi.m set up the dataset for the problems and make calls to functions
in the files listed above.

## Motivation

This is a great reference for good practices and what's "under the hood" in multivariate linear regression.

## Acknowledgements

This project is part of the Stanford University Machine Learning course on Coursera, instructed by Andrew Ng.
